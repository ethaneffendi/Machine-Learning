<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="" />
  <title>Supervised Learning</title>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <link href="style.css" rel="stylesheet" type="text/css">
</head>

<body>

  <hr>

  <!--Supervised Learning-->
  <h1 class="text">Supervised Learning</h1>
  <p class="text">
    Supervised learning is the macihine learning setup that involves a dataset organized into pairs of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values: <span class="math inline">\(D_n = \{(x^{(1)}, y^{(1)}), ... , (x^{(n)}, y^{(n)})\}\)</span>. Some mapping must be learned between the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values so that given a new <span class="math inline">\(x\)</span> encountered in the future, a computer can accurately predict the corresponding <span class="math inline">\(y\)</span>. For example, <span class="math inline">\(x\)</span> values might be a patient's vital signs and <span class="math inline">\(y\)</span> values might be whether or not that patient is having a heart attack.
  </p>
  <p class="text">
    <span class="math inline">\(x\)</span> values are vectors in <span class="math inline">\(d\)</span> dimensions: <span class="math inline">\(x^{(i)} \in \mathbb{R}^d\)</span>. The set <span class="math inline">\(y\)</span> values belong to can change depending on what problem is being approached. However, when the goal is to classify <span class="math inline">\(x\)</span> values (whether or not a patient's vital signs indicate a heart attack or not), the set <span class="math inline">\(y\)</span> belongs to should contain discrete numbers. In particular, if there are only two categories <span class="math inline">\(x\)</span> can be classified as, then one is working with binary classification: <span class="math inline">\(y^{(i)} \in \{+1, -1\}\)</span>.
  </p>
  <p class="text">
    The relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values that a computer learns is called a hypothesis. It is some function that takes in an <span class="math inline">\(x\)</span> and returns a <span class="math inline">\(y\)</span>. Hypothesis also have parameters <span class="math inline">\(\Theta\)</span>, but they will be elaborated on later. A hypothesis is written as <span class="math inline">\(y = h(x; \Theta)\)</span>.
  </p>
  <p class="text">
    How does one know that the prediction a hypothesis makes is accurate though? Loss functions are the answer. They are written as <span class="math inline">\(L(g,a)\)</span>. The loss function takes in a hypothesis's guess <span class="math inline">\(g\)</span>. <span class="math inline">\(g\)</span> is basically just the value the hypothesis predicts given some <span class="math inline">\(x\)</span> it encounters. <span class="math inline">\(a\)</span> is the correct value that the hypothesis should return. The loss function returns how sad one should be that <span class="math inline">\(g\)</span> was guessed when <span class="math inline">\(a\)</span> was the actual answer. Because being sad is not fun, lower loss is better. A plethora of different loss functions exist.
  </p>
  <ul class="text">
    <li>squared loss: <span class="math inline">\(L(g,a) = (g-a)^2\)</span></li>
    <li>linear loss: <span class="math inline">\(L(g,a) = |g-a|\)</span></li>
    <li>0-1 loss: <span class="math inline">\(
        L(g,a)=
        \begin{cases}
            0 & \text{if } g = a \\
            1 & \text{otherwise}
        \end{cases}
    \)</span>
      </li>
  </ul>
  <p class="text">
    Now, how can one measure how well a hypothesis works? To begin, the hypothesis should work well on the data it was trained on. The training set error allows measures the average loss of the hypothesis on the training data: <span class="math inline">\(E_n(h) = {1\over{n}}\sum^{n}_{i=1}{L(h(x^{(i)}),y^{(i)})}\)</span>. The hypothesis can be likened to a student in a calculus class though. Just memorizing homework answers does not guarantee the student full marks on the final exam. Think of the training set like a bunch of homework problems. The student should be able to generalize from practice and apply skills to test questions on final exam day. To really understand how well a hypothesis will perform on new data, one should save out a portion of the training data and call it the testing set. The test error is the average of the hypothesis's losses on the test set data: <span class="math inline">\(E(h) = {1\over{n'}}\sum^{n+n'}_{i=n+1}L(h(x^{(i)}),y^{(i)})\)</span>.
  </p>
  <p class="text">
    The computer learns hypotheses from datasets by using learning algorithms. However, learning algorithms will be explained more extensively later.
  </p>
  <hr>

  <!--Linear Classifiers-->
  <h1 class="text">Linear Classifiers</h1>
  <p class="text">
    A linear classifier is a type of hypothesis for the supervised learning setup. To visualize how linear classifiers work, one must plot all <span class="math inline">\(x\)</span> values in <span class="math inline">\(D_n\)</span> onto the <span class="math inline">\(\mathbb{R}^d\)</span> space. Each axis of the <span class="math inline">\(\mathbb{R}^d\)</span> space (i.e. <span class="math inline">\(x_1, x_2, ... , x_n\)</span>) corresponds to a dimension of the vector <span class="math inline">\(x\)</span>. The first dimension of <span class="math inline">\(x\)</span> is its first entry, the second dimension is its second entry, and so on. Some of the plotted points are assigned a <span class="math inline">\(y\)</span> value of <span class="math inline">\(+1\)</span> whereas others are assigned to <span class="math inline">\(-1\)</span>.
  </p>
  <figure>
    <img src="(2) Linear Classifiers/Plotted X Values.png" alt="Plotted X Values" width="50%">
  </figure>
  <p class="text">
    Once the dataset is visualized on a graph, how to write a classifier becomes really obvious! For the two-dimensional space above, one just needs to draw a line that separates <span class="math inline">\(\mathbb{R}^2\)</span> into a <span class="math inline">\(+1\)</span> subspace and <span class="math inline">\(-1\)</span> subspace. In other words, one just needs to find a line such that all points that were assigned <span class="math inline">\(+1\)</span> in the dataset sit on one side while all points that were assigned <span class="math inline">\(-1\)</span> sit on the other side. The separator that does the job is called a linear classifier.
  </p>
  <figure>
    <img src="(2) Linear Classifiers/Classified X Values.png" alt="Classified X Values" width="50%">
  </figure>
  <p class="text">
    For a dataset with two-dimensional <span class="math inline">\(x\)</span> vectors, linear classifiers, as described earlier, are just lines. For a dataset with three-dimensional <span class="math inline">\(x\)</span> vectors, linear classifiers are planes. When the dataset contains <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\(x\)</span> vectors, the general term used to describe the separator that classifies them is a hyperplane. For the <span class="math inline">\(\mathbb{R}^n\)</span> space, hyperplanes are a space with <span class="math inline">\(n-1\)</span> dimensions. A hyperplane has a normal that points in the direction of the <span class="math inline">\(+1\)</span> subspace.
  </p>
  <p class="text">
    A linear classifier may be written formally as:
    <span class="math inline">
      \(h(x; \theta, \theta_0) = \text{sign}(\theta^T{x} + \theta_0) =
      \begin{cases}
          +1, & \theta^T x + \theta_0 > 0 \\
          -1, & \text{otherwise}
      \end{cases}\)
    </span>.
    Visibly, it has two parameters <span class="math inline">\(	\theta\)</span> and <span class="math inline">\(	\theta_0\)</span>. Learning algorithms try to find the parameters that will construct a separator that accurately classifies <span class="math inline">\(+1\)</span> from <span class="math inline">\(-1\)</span>. <span class="math inline">\(	\theta \in \mathbb{R}^d\)</span> and <span class="math inline">\(	\theta_0\in\mathbb{R}\)</span>.
  </p>
  <p class="text">
    The simplest learning algorithm that one can begin with is the random linear classifier algorithm. The algorithm works by producing random parameters <span class="math inline">\(k\)</span> times. At the end, it returns the parameters with the lowest training set error. <span class="math inline">\(k\)</span> is called a hyperparameter. It is not a parameter of the hypothesis. Rather, it is a parameter of the learning algorithm. It impacts how training occurs.
  </p>
  <p>
      Algorithm: Random Linear Classifier <br>
      Input: \(D_n\), \(k\) <br>
      for \(j = 1\) to \(k\): <br>
      &emsp; \( \theta^{(j)} \gets \text{random}(\mathbb{R}^d) \) <br>
      &emsp; \( \theta_0^{(j)} \gets \text{random}(\mathbb{R}) \) <br>
      end for <br>
      \( j^* \gets \arg\min_{j \in \{1, ..., k\}} {E_n(h(\cdot,\theta,\theta_0))} \) <br>
      return \( \theta^{(j^*)}, \theta_0^{(j^*)} \) <br>
  </p>
  <script src="https://gist.github.com/ethaneffendi/06cf8f947ad9c15482f53036f64a3050.js"></script>
  <hr>

  <!--Perceptron-->
  <h1 class="text">Perceptron</h1>
  <pclass="text">Perceptron is a more sophisticated learning algorithm for producing linear classifiers.</p>
  <p>
    Input: \(D_n\), \(T\) <br>
    \(\theta = \overline{0}\) <br>
    \(\theta_0 = 0\) <br>
    For \(t = 1\) to \(T\) <br>
    &emsp;For \(i=1\) to \(n\) <br>
        &emsp;&emsp;If \(y^{(i)}(\theta^Tx^{i}+\theta_0)\leq0\) <br>
            &emsp;&emsp;&emsp;\(\theta = \theta + y^{(i)}x^{(i)}\) <br>
            &emsp;&emsp;&emsp;\(\theta_0 = \theta_0+y^{(i)}\) <br>
    Return \(\theta, \theta_0\)
  </p>
  <p class="text">The algorithm iterates through the dataset \(D_n\) \(T\) times (\(T\) is a hyperparameter). At each point in the dataset, it checks if \(y^{(i)}(\theta^Tx^{(i)}+\theta_0)\leq0\). The if statement basically determines whether or not the current \(\theta\) classified the point at index \(i\) correctly. If \((x^{(i)}, y^{(i)})\) were correctly classified, then \(y^{(i)}(\theta^Tx^{(i)}+\theta_0)\) would be positive. Recall that \(\theta^Tx^{(i)}+\theta_0\) is the input to the sign function in the linear classifier hypothesis. Thus, if it is negative and \(y^{(i)}\) is also negative, their product should be positive. If both are positive, their product is positive too. If they are of different signs (an incorrect classification occurred), then the if statement would trigger some modifications. In particular, it modifies \(\theta\) to \(\theta+y^{(i)}x^{(i)}\) and \(\theta_0\) to \(\theta_0+y^{(i)}\). </p>
  <p class="text">The modifications that perceptron makes to the parameters \(\theta\) and \(\theta_0\) probably are not the obvious “right move.” One is probably left wondering why Rosenblatt, the inventor of perceptron, chose to set \(\theta=\theta+y^{(i)}x^{(i)}\) and \(\theta_0=\theta_0+y^{(i)}\). This brings up another interesting discussion point. Whereas the inner workings of most algorithms have been built intuitively around the problem to be solved, perceptron was simply introduced and left for scholars to analyze over the decades. Years of papers have determined that Rosenblatt's modifications are quite functional. </p>
  <p class="text">To better analyze how perceptron works and even introduce a theorem about it, exploring a simpler version of the algorithm is useful. Think of perceptron-through-origin as perceptron without any offset (\(\theta_0\)) parameter. Some playing with dimensions later on will show that what applies to perceptron-through-origin applies to perceptron with an offset.</p>
  <h2 class="text">Linear Separability</h2>
  <p class="text">Linear separability is a property of a dataset \(D_n\). \(D_n\) is linearly separable when there exists some \(\theta\) (no \(\theta_0\) because the current discussion is about perceptron-through-origin) such that \(y^{(i)}(\theta^Tx^{(i)})>0\) for all \(i\). In other words, all points in the dataset \(D_n\) are correctly classified.</p>
  <figure>
      <img src="(3) Perceptron/Linear Separability.png" alt="Linear Separability" style="width:50%;">
  </figure>
  <h2 class="text">Margin</h2>
  <p class="text">The margin of a data point \((x,y)\) with repsect to a linear separator (a hyperplane) is \(y{{\theta^Tx}\over{||\theta||}}\). \({{\theta^Tx}\over{||\theta||}}\) is the signed distance from the point to the separator. If \(y\) is \(-1\) and \((x,y)\) is correctly classified, then \({{\theta^Tx}\over{||\theta||}}\) should be negative. \(y\) is the target label (either \(+1\) or \(-1\)). Therefore, a correctly classified point should have a positive margin. An incorrectly classified point should have a negative margin. A higher margin is better because the farther a point is away from the classifier, the better classified it is (not a close call).</p>
  <figure>
      <img src="(3) Perceptron/Margin of a Point.png" alt="Margin of a Point" style="width:50%;">
  </figure>
  <p class="text">The margin of an entire dataset with respect to a linear separator is equal to the margin of the point it contains that has the lowest margin: \(\min_{i}{y^{(i)}{{\theta^Tx^{(i)}}\over{||\theta||}}}\).</p>
  <h2 class="text">Perceptron Convergence Theorem</h2>
  <p class="text">The perceptron convergence states that if (a) there exists some \(\theta^*\) such that \(y^{(i)}{{\theta^Tx^{(i)}}\over{||\theta||}}>\gamma>0\) for all \(i\) (in other words, if the margin of the dataset with respect to \(\theta^*\)) is greater than or equal to some constant \(\gamma\)) and (b) \(||x^{(i)}||\leq{R}\) for all \(i\) (in other words, when graphed, the data points are contained in a circle of radius \(R\)), then the perceptron will make at most \(\left({{R}\over{\gamma}}\right)^2\) modifications during training.</p>
  <h2 class="text">Proof of the Perceptron Convergence Theorem</h2>
  <p class="text">Say that \(\theta^{(k)}\) is the hypothesis produced after \(k\) modifications during training and \(\theta^*\) is the parameter such that \(y^{(i)}{{\theta^{*T}x^{(i)}}\over{||\theta^*||}}\geq\gamma>0\) for all \(i\). The angle between \(\theta^{(k)}\) and \(\theta^*\) is \(\alpha\). To show that \(\theta^{(k)}\) will converge to become \(\theta^*\) as \(k\) increases, one must show that the angle \(\alpha\) becomes smaller and smaller. As the angle becomes smaller, its cosine should become greater (i.e. \(\cos{\alpha}\) should increase).</p>
  <ol class="text">
      <li>There is a formula that describes the cosine of the angle \(\alpha\) between two vectors: \(\cos{\alpha}={{a\cdot{b}}\over{||a|||b||}}\). Thus, \(\cos{\alpha}={{\theta^*\cdot\theta^{(k)}}\over{||\theta^*||||\theta^{(k)}||}}\).</li>
      <li>One can break \({{\theta^*\cdot\theta^{(k)}}\over{||\theta^*||||\theta^{(k)}||}}\) down into \({{\theta^*\cdot\theta^{(k)}}\over{||\theta^*||}}\cdot{1\over{||\theta^{(k)}||}}\).</li>
      <li>First, one should analyze \({{\theta^*\cdot\theta^{(k)}}\over{||\theta^*||}}\). \(\theta^{(k)}=(\theta^{(k-1)}+y^{(i)}x^{(i)})\) when \(i\) is the index of the point at which perceptron made its last modification to \(\theta\). Using this information, \({{\theta^*\cdot\theta^{(k)}}\over{||\theta^*||}}={{(\theta^{(k-1)}+y^{(i)}x^{(i)})\theta^*}\over{||\theta^*||}}={{\theta^{(k-1)}\cdot\theta^*}\over{||\theta^*||}}+{{y^{(i)}x^{(i)}\cdot\theta^*}\over{||\theta^*||}}\). Recall that \({{y^{(i)}x^{(i)}\cdot\theta^*}\over{||\theta^*||}}\) is the margin, which is greater than or equal to \(\gamma\). Thus, \({{\theta^{(k-1)}\cdot\theta^*}\over{||\theta^*||}}+{{y^{(i)}x^{(i)}\cdot\theta^*}\over{||\theta^*||}}\geq{k\gamma}\).</li>
      <li>Now, one may analyze \({1\over{||\theta^{(k)}||}}\). \(||\theta^{(k)}||^2 = ||\theta^{(k-1)}+y^{(i)}x^{(i)}||^2 = ||\theta^{(k-1)}||^2 + 2y^{(i)}\theta^{(k-1)}\cdot{x^{(i)}} + ||x^{(i)}||^2\). It is given in the perceptron convergence theorem that \(||x^{(i)}||^2 \leq R^2\). Also, \(2y^{(i)}\theta^{(k-1)}\cdot{x^{(i)}}\) is negative because \(\theta^{k}\) made a mistake. Therefore, \(||\theta^{(k-1)}||^2 + 2y^{(i)}\theta^{(k-1)}\cdot{x^{(i)}} + ||x^{(i)}||^2 \leq {kR^2}\). Thus, \({1\over||\theta^{(k)}||}\geq{1\over{\sqrt{k}R}}\).</li>
      <li>Putting everything together, \(\cos{\alpha} \geq {\sqrt{k}}{{\gamma}\over{R}}\). Provided that cosine's greatest output is 1, \(1\geq{{\sqrt{k}\gamma}\over{R}}\), so \(k\leq{({{R}\over{\gamma}})^2}\).</li>
  </ol>
  <script src="https://gist.github.com/ethaneffendi/a8ae1c5ad9f4579f8dfdee864de89b3e.js"></script>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/-sMDcITU-ng?si=zfauEPeE5YMwwJRO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  <hr>

  <!--Polynomial Basis-->
  <h1 class="text">Polynomial Basis</h1>
  <h2 class="text">Reducing Perceptron-Not-Through-Origin to Perceptron-Through-Origin</h2>
  <p class="text">
    As promised earlier, the problem of perceptron-not-through-origin can be reduced to the problem of perceptron-through-origin. The key lies in transforming the dataset used from the \( \mathbb{R}^d \) space to a \( \mathbb{R}^D \) space when \( D > d \).
  </p>
  <p class="text">
    Recall that the parameters of a separator not through the origin are \( \theta=[\theta_1, ... \theta_d] \) and \( \theta_0 \), a scalar. Say that these two parameters are taken to create a single one called \( \theta_{new} \): \( \theta_{new} = [\theta_1, ... \theta_d, \theta_0] \). \( \theta_{new} \) is basically just \( \theta \) with \( \theta_0 \) put in as the last entry. Now, each \( x \) value vector in the dataset must also be modified. Say that \( x_{new} = [x_1, ...,x_d,1] \). \( x_{new} \) is basically just \( x \) with \( 1 \) put in as the last entry. \( \theta^T_{new}x_{new} = \theta_1x_1 + ... + \theta_dx_d+(1)\theta_0=\theta_1x_1+...+\theta_dx_d+\theta_0=\theta^Tx+\theta_0 \). A classifier-not-through-origin in \( d \) dimensions can be turned into a classifier-through-origin in \( d+1 \) dimensions. Thus, even though the perceptron convergence theorem from before was only proved for perceptron through origin, transformation shows that the theorem also applies to perceptron-not-through-origin.
  </p>
  <figure>
    <img src="(4) Polynomial Basis/Perceptron-Not-Through-Origin to Perceptron-Through-Origin.png" alt="Perceptron-Not-Through-Origin to Perceptron-Through-Origin" style="width:50%;">
  </figure>
  <h2 class="text">Polynomial Basis</h2>
  <p class="text">
    The same concept can be applied to a dataset that is not linearly separable in \( d \) dimensions. By moving the dataset into \( d+1 \), \( d+2 \), or more generally, \( d+n \) dimensions, one can make it linearly separable.
  </p>
  <figure>
    <img src="(4) Polynomial Basis/Linearly Separable in Higher Dimensions.png" alt="Linearly Separable in Higher Dimensions" style="width:50%;">
  </figure>
  <p class="text">
    A systematic way of transforming data into higher dimensions exists. It is called polynomial basis.
  </p>
  <div style="text-align: center;" class="text">
    <table>
      <tr>
        <td>order</td>
        <td>in general</td>
      </tr>
      <tr>
        <td>0</td>
        <td>\([1]\)</td>
      </tr>
      <tr>
        <td>1</td>
        <td>\([1, x_1, ... x_d]\)</td>
      </tr>
      <tr>
        <td>2</td>
        <td>\([1, x_1, ... x_d, x_1^2, x_1x_2, ... \text{all two way products}]\)</td>
      </tr>
      <tr>
        <td>3</td>
        <td>\([1, x_1, ... x_d, x_1^2, x_1x_2, ... \text{all two way products}, x_1^3, ... \text{all three way products}]\)</td>
      </tr>
      <tr>
        <td>...</td>
        <td>...</td>
      </tr>
    </table>
  </div>
  <p class="text">
    For an example, take \([x_1, x_2]\). Using polynomial basis to transform it to the second degree yields \([1, x_1, x_2, x_1^2, x_1x_2, x^2]\).
  </p>
  <hr>

  <!--Logistic Regression & Gradient Descent-->
  <h1 class="text">Logistic Regression and Gradient Descent</h1>
  <p class="text">Another type of learning algorithm is one based on optimization. In minimizing some function \(J\), called an objective function, the learning algorithm should in effect minimize the training set error on \(D_n\) of the classifier it produces.  </p>
  <p class="text">Objective function \(J\) should be written as \(J(\Theta) = {1\over{n}}{\sum^n_{i=1}}{L(h(x^{(i)}, \Theta),y^{(i)})+\lambda{R(\Theta)}}\). The first addend is the training set error of hypothesis \(h\). The second addend is a new concept. \(R(\Theta)\) is a regularizer. It prevents the learning algorithm from being too honed in on minimizing training set error. Sometimes, learning algorithms that are too focused on minimizing training set error produce a classifier that works too specifically well on training data that it is out of touch with the real-life data it will be thrown in the future. \(\lambda\) is a positive constant (a hyperparameter) that determines to what degree the regularizer will be considered. Also, one must know that \(\Theta\) is used to generally represent all parameters of \(h\). For the scope of logistic regression, \(\Theta\) encompasses \(\theta\) and \(\theta_0\).  </p>
  <p class="text">The type of hypothesis produced in linear regression is different from a standard linear classifier. The hypothesis in linear regression is written as \(h(x; \theta, \theta_0)=\sigma(\theta^Tx+\theta_0)\) when \(\sigma\) is the sigmoid function. The sigmoid function is defined as \(\sigma(z)={{1}\over{1+e^{-z}}}\). Why can't one use a standard linear classifier and the zero-one loss function in the objective function to be optimized? Zero-one loss only says whether or not a separator classifies a point correctly or incorrectly. On the other hand, because \(\sigma(z)\in(0,1)\) and \(\sigma(0)=0.5\), the sigmoid function says how correctly or incorrectly a separator classifies a point. These semantics are what optimization takes advantage of in working towards the optimal set of parameters.</p>
  <figure>
    <img src="(6) Logistic Regression & Gradient Descent/Sigmoid Function.png" alt="Sigmoid Function" style="width:50%;">
  </figure>
  <p class="text">To actually classify points using \(h(x; \theta, \theta_0)=\sigma(\theta^Tx+\theta_0)\) though, one needs to interpret the output of \(\sigma\). For instance, if the threshold is 0.5, all outputs of \(\sigma\) greater than 0.5 will be considered +1 whereas all outputs less than or equal to 0.5 will be negative. However, because the bounds of the sigmoid function's range are 0 and 1, \(y^{(i)}\in{0,1}\) in the dataset. The most common setup is for the threshold to be 0.5. In that case, wherever \(\sigma(z)=0.5\) is the linear classifier.</p>
  <figure>
    <img src="(6) Logistic Regression & Gradient Descent/Classifier Using Sigmoid Function.png" alt="Classifier Using Sigmoid Function" style="width:50%;">
  </figure>
  <p class="text">To visualize this phenomenon in the two dimensional space, imagine a plane curved like the sigmoid function sticking out through the third axis. The separator is where the sigmoid axis is 0.5.</p>
  <h2 class="text">Negative Log Likelihood Loss</h2>
  <p class="text">The loss function that goes with the hypothesis logistic regression produces is called negative log likelihood loss. Deriving the function will allow one to make good sense of it.</p>
  <ol class="text">
    <li>The probability that \(\theta\) and \(\theta_0\) classify the dataset \(D_n\) can be written as \(\prod^n_{i=1}{\begin{cases}
                g^{(i)},\ y^{(i)}=1 \\
                1-g^{(i)} ,\ y^{(i)}=0
            \end{cases}}=\prod^n_{i=1}({{g^{(i)}}^{y^{(i))}}+(1-g^{(i)})^{(1-y^{(i)})}})\).
            Each data point being classified is an independent event. Therefore, the probability of them all being classified correctly is the product of each point's probability of being correctly classified.</li>
    <li>Sums are easier to work with than products. Quite conveniently, the logarithm of a product is a sum. Also, because \(\log{x}\) is monotonic, minimizing \(\log{x}\) minimizes \(x\). Thus, one should just work with \(\log{(\prod^n_{i=1}({{g^{(i)}}^{y^{(i))}}+(1-g^{(i)})^{(1-y^{(i)})}})})\). Computing the logarithm produces \(\sum^n_{i=1}({y^{(i)}\log{g^{(i)}}+(1-y^{(i)})\log{(1-g^{i})}})\).</li>
    <li>Even though the sum works for maximizing the probability that \(\theta\) and \(\theta_0\) classify the data, the job is still to design a loss functions. Loss functions should be minimized. The simple solution is negating the sum: \(-\sum^n_{i=1}({y^{(i)}\log{g^{(i)}}+(1-y^{(i)})\log{(1-g^{i})}})\).</li>
    <li>The loss function will simply be the summand of the summation: \(L(g,a)=-(a\log{g}+(1-a)\log{(1-g)})\).</li>
  </ol>
  <p class="text">Negative log likelihood loss is also called cross entropy loss or log loss.  </p>
  <h2 class="text">Regularizer</h2>
  <p class="text">A common regularizer is \(R(\Theta)=||\Theta||^2\). It tries to keep the norms of the parameters small during training. As a result, the learning algorithm will not try too hard to produce a classifier that does well on outlier points. The regularizer also prevents the classifier from overfitting the provided training data.</p>
  <h2 class="text">Learning Algorithm for Logistic Regression: Gradient Descent</h2>
  <p class="text">Gradient descent is a common optimization learning algorithm for logistic regression. To understand how gradient descent works, one can begin by observing it in one dimension.  </p>
  <p class="text">Say there is some function \(f\), some starting position called \(x_{init}\), and some step size \(\eta\). The algorithm obtains the derivative of \(f\) at \(x_{init}\). It then moves in the negative direction of the derivative. The size of the step it takes is determined by \(\eta\). Eventually, after enough moving around, gradient descent should find a relative minimum of \(f\).  </p>
  <figure>
    <img src="(6) Logistic Regression & Gradient Descent/Gradient Descent.png" alt="Gradient Descent" style="width:50%;">
  </figure>
  <p class="text">In the figure above, \(x\) will continue bouncing back and forth until its movements are less than some tolerance constant \(\epsilon\). </p>
  <h2 class="text">Partial Derivatives and Gradients</h2>
  <p class="text">Examine the multivariable function \(f(x,y) = x^2y\). The partial derivative \(\frac{\partial{x}}{\partial{f}}\) is the derivative of \(x^2y\) when y is treated like a constant. The same is true for \(\frac{\partial{y}}{\partial{f}}\), except \(x\) is treated like a constant. Neither partial derivative tells the entire rate of change of \(f\), but they do give insight into how \(f\) changes as one of its variables does. The gradient of \(f\), written as \(\nabla{f}\) is the vector of all partial derivatives of \(f\):
        \(\begin{bmatrix}
               \frac{\partial{x}}{\partial{f}}
               \frac{\partial{y}}{\partial{f}}
        \end{bmatrix}\).</p>

  <h2 class="text">Gradient Descent in Multiple Dimensions</h2>
  <div class="algorithm">
  <p>
      Algorithm: Gradient Descent in Multiple Dimensions <br>
      Input: \( \theta_{init} \), \( f \), \( \nabla_\theta{f} \), \( \epsilon \), \( \eta \) <br>
      \( \theta^{(0)} = \theta_{init} \) <br>
      \( t = 0 \) <br>
      While \( f(\theta^{(t)}) - f(\theta^{(t-1)}) \geq \epsilon \): <br>
      &emsp; \( t = t + 1 \) <br>
      &emsp; \( \theta^{(t)} = \theta^{(t-1)} - \eta \nabla{f(\theta^{(t-1)})} \) <br>
      Return \( \theta \) <br>
  </p>
  </div>
  <h2 class="text">Gradient Descent for Logistic Regression</h2>
  <p class="text">Gradient descent for logistic regression uses the listed formulas. </p>
  <p class="text">\(J(\theta, \theta_0) = {1\over{n}}{\sum_{i=1}^n{L(\sigma(\theta^Tx+\theta_0),y^{(i)})}+\frac{\lambda}{2}{||\theta||^2}}\).  </p>
  <p class="text">\(\nabla_\theta{J(\theta, \theta_0)}=\frac{1}{n}\sum_{i=1}^{n}{(\sigma(\theta^Tx+\theta_0),y^{(i)})-y^{(i)})x^{(i)}}+\lambda\theta\)</p>
  <p class="text">\(\frac{\partial J(\theta, \theta_0)}{\partial \theta_0}={1\over{n}}\sum^n_{i=1}({\sigma(\theta^Tx+\theta_0)}-y^{(i)})\)  </p>
  <p class="text">The algorithm is the same, but the edits are the subtraction of the gradient above and the partial derivative above from \(\theta_{(t)}\) and \(\theta_0^{(t)}\) inside the loop.</p>
  <script src="https://gist.github.com/ethaneffendi/4918217721ab695f825ce67bbdd5f297.js"></script>
  <hr>

  <!--Regression-->
  <h1 class="text">Regression</h1>
  <p class="text">
    Like classification, regression is a type of supervised learning setup. Thus, its dataset is still \(D_n = \{(x^{(1)}, y^{(1)}), ... ,(x^{(n)}, y^{(n)})\}\). \(x^{(i)}\in\mathbb{R}^d\), but the set \(y^{(i)}\) belongs to will change: \(y^{(i)}\in\mathbb{R}\). Whereas the \(y\) values in classification were discrete categories, the \(y\) values in regression are continuous values. For example, one might use regression to predict a company's stock price or the temperature in two days. Neither stock price nor temperature should be predicted using categories like \(\{+1, -1\}\); they are continuous values. <br><br>
    The hypothesis for linear regression (finding a trend line) is written as \(h(x; \theta, \theta_0) = \theta^Tx+\theta_0\) when \(\theta\in\mathbb{R}^d\) and \(\theta_0\in\mathbb{R}\). <br><br>
    The standard loss function that is conventionally used for linear regression is called squared loss: \(L(g, a) = (g-a)^2\). <br><br>
    The regression setup involving the linear regression hypothesis and squared loss function is so common that it has earned a name: ordinary least squares. <br>
  </p>
  <h2 class="text">Using Optimization to Find a Hypothesis</h2>
  <p class="text">
    Quite naturally, upon seeing \(h(x;\theta,\theta_0)=\theta^Tx+\theta_0\), one should want to use optimization to learn the best hypothesis for the dataset \(D_n\). Thus, an objective function must be written. To highlight how important regularizers are, \(J\) will first be written without \(\lambda{R(\Theta)}\): \(J(\theta, \theta_0)=\frac{1}{n}\sum^n_{i=1}{(\theta^Tx^{(i)}-\theta_0-y^{(i)})^2}\). The optimal parameters \(\theta_0^*\) and \(\theta^*\) are \(\arg\min_{\theta, \theta_0}{J(\theta, \theta_0)}\). The calculus way of finding a minimum will work: find the derivative of the objective function, set it to 0, solve for critical points, and prove that critical points are minima. One can most definitely compute the derivative of \(J\) with respect to each entry of \(\theta\) and whatnot to produce a system of equations that can be solved for optimal parameters, but using the matrix form of the system is more elegant. Say that \(W=
    \begin{bmatrix}
    x^{(1)}_1 & ... & x^{(1)}_d & 1\\
    \vdots{} & \ddots & & \vdots{} \\
    x^{(n)}_1 & ... & x^{(n)}_d & 1
    \end{bmatrix}\). Each \(x\) value in \(D_n\) is turned into a row of \(W\). Each row has a \(1\) added to its end in order to account for \(\theta_0\) (a separator-through-origin in \(d\) dimensions can be turned into a separator-not-through-origin in \(d-1\) dimensions). Also, \(T=\begin{bmatrix}
    y^{(1)}\\
    \vdots \\
    y^{(d)}
    \end{bmatrix}\). Now, one can use linear algebra to find the parameter array \(\theta_{array}\) that minimizes a rewritten objective function \(J(\theta_{array})\) instead of finding the best \(\theta\) vector and \(\theta_0\) scalar for the original \(J(\theta, \theta_0)\). \(J(\theta,\theta_0)=J(\theta_{array})=(W\theta_{array}-T)^T(W\theta_{array}-T).\) Now, one needs to find the gradient (vector of partial derivatives) of \(J(\theta_{array})\), set it to \(0\), and solve for \(\theta_{array}\). \(\nabla_{\theta_{array}}J=\frac{2}{n}W^T(W\theta_{array}-T)\). \(0=\frac{2}{n}W^T(W\theta_{array}-T)\), \(0=W^T(W\theta_{array}-T)\), \(0=W^TW\theta_{array}-W^TT\), \(W^TW\theta_{array}=W^TT\), \(W^TW\theta_{array}=W^TT\), \(W^TW\theta_{array}=W^TT\), \(W^TW\theta_{array}=W^TT\), \((W^TT)^{-1}W^TW\theta_{array}=(W^TT)^{-1}W^TT\), so \(\theta_{array}^*=(W^TT)^{-1}W^TT\). This is an example of the rare closed form solution. One does not need a fancy computer or algorithm to use a closed form solution. It simply describes how to solve for \(\theta_{array}^*\) using pencil and paper. The problem with \(\theta_{array}^*=(W^TT)^{-1}W^TT\) is that \((W^TT)\) might not be invertible such that \((W^TT)^{-1}\) can be computed. The way of overcoming this problem is to add the regularizer \(||\theta||^2\) to \(J\) to create an objective function called the ridge regression objective function. \(J_{ridge}(\theta, \theta_0)=[\frac{1}{n}{\sum_{i=1}^n{(\theta^Tx+\theta_0-y^{(i)})^2}}]+\lambda{||\theta||^2}\). The regularizer does not impact \(\theta_0\) because offsets do not have to be forced toward \(0\) to prevent overcomplicated parameters. \(\nabla_{\theta_{array}{J_{ridge}}}=\frac{2}{n}W^T(W\theta_{array}-T)+2\lambda\theta_{array}\). Solving for \(0\) yields \(\theta_{array}^*=(W^TW+n\lambda{I})^{-1}W^TT\). \(I\) is the identity matrix (the values along its diagonal are all set to 1 while the rest of the matrix is set to 0). Its diagonal of 1s being multiplied by \(n\) and \(\lambda>0\) adds weight to the diagonal (ridge) of \(W^TT\), making it invertible.\<br><br>
    Inverting \(W^TW+n\lambda{I}\) takes \(O(d^3n)\) time, so for high dimension datasets, gradient descent or stochastic gradient descent will be better for performance than the analytic method described above. For gradient descent or stochastic gradient descent, one must know that \(\nabla_{\theta_{array}}J_{ridge}=\frac{2}{n}\sum_{i=1}^n{(\theta_{array}^Tx^{(i)}+\theta_0-y^{(i)})x^{(i)}}+2\lambda{\theta_{array}}\) and \(\frac{\partial{J}}{\partial\theta_{array}}=\frac{2}{n}{\sum_{i=0}^{n}}(\theta^Tx^{(i)}+\theta_0-y^{(i)})\).
  </p>
  <script src="https://gist.github.com/ethaneffendi/d43edfc0ad9adf300276c0c0d69fc115.js"></script>
  <hr>

</body>
</html>
